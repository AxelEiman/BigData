{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from time import perf_counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Prediction quality vs feature selection\n",
    "\n",
    "#### Tasks\n",
    "\n",
    "In this taks you are supposed to\n",
    "\n",
    "1. repeatedly simulate new datasets and for each dataset do Steps 2 and 3 \n",
    "2.  determine λ_{min} and λ_{1se} using cross-validation on the training data\n",
    "\n",
    "3.  compare the two resulting Lasso models with respect to\n",
    "    - Mean squared error on the training as well as test data\n",
    "    - Feature selection quality in comparison to the original simulated regression coefficients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look a bit closer at each task.\n",
    "\n",
    "1. Simulate data\n",
    "Simulating useful data for the Lasso isn't complicated but to make sure you can focus on the interesting tasks I included a Python and a R version of a data-generating function. You can either ignore them and follow the description here, use them or take them as inspiration to write your own. The ideas are\n",
    "\n",
    "    1. Generate n observations from an isometric normal distribution $N(0,Ip)$, i.e. theoretically uncorrelated features.\n",
    "    2. Generate regression coefficients beta as a vector that contains ceiling((1 - sparsity) p) non-zero elements [ceiling = closest integer less or equal to] that are normal distributed with a standard deviation of beta_scale. All other elements are zero.\n",
    "    3. At this point Xβ is the noise-less response and √∥Xβ∥22/(n−1) is its sample standard deviation. Signal-to-noise ratio is a measure for the proportion of the signal (noise-less response) standard deviation to the standard deviation of the noise, i.e. SNR = sd_signal / sd_noise. Specifying the SNR is a convenient way to determine what standard deviation for the noise is reasonable by choosing sd_noise = sd_signal / SNR. As an example, SNR = 2 means that the standard deviation of the noise-less is twice as large as the standard deviation of the noise and the noise-less response will be more \"pronounced\" the larger the SNR is. If 0 <= SNR < 1, then the noise is stronger than the signal which is hard to deal with for most methods.\n",
    "    4. Finally, the response is created in the form of a linear model y = X beta + sigma eps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_data(n, p, rng, *, sparsity=0.95, SNR=2.0, beta_scale=5.0):\n",
    "    \"\"\"Simulate data for Project 3, Part 1.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Number of samples\n",
    "    p : int\n",
    "        Number of features\n",
    "    rng : numpy.random.Generator\n",
    "        Random number generator (e.g. from `numpy.random.default_rng`)\n",
    "    sparsity : float in (0, 1)\n",
    "        Percentage of zero elements in simulated regression coefficients\n",
    "    SNR : positive float\n",
    "        Signal-to-noise ratio (see explanation above)\n",
    "    beta_scale : float\n",
    "        Scaling for the coefficient to make sure they are large\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : `n x p` numpy.array\n",
    "        Matrix of features\n",
    "    y : `n` numpy.array\n",
    "        Vector of responses\n",
    "    beta : `p` numpy.array\n",
    "        Vector of regression coefficients\n",
    "    \"\"\"\n",
    "    X = rng.standard_normal(size=(n, p))\n",
    "    \n",
    "    q = int(np.ceil((1.0 - sparsity) * p))\n",
    "    beta = np.zeros((p,), dtype=float)\n",
    "    beta[:q] = beta_scale * rng.standard_normal(size=(q,))\n",
    "    \n",
    "    sigma = np.sqrt(np.sum(np.square(X @ beta)) / (n - 1)) / SNR\n",
    "\n",
    "    y = X @ beta + sigma * rng.standard_normal(size=(n,))\n",
    "\n",
    "    # Shuffle columns so that non-zero features appear\n",
    "    # not simply in the first (1 - sparsity) * p columns\n",
    "    idx_col = rng.permutation(p)\n",
    "    \n",
    "    return X[:, idx_col], y, beta[idx_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 500     # Fix p at something large, e.g. 500 or 1000\n",
    "n_list = [125, 250, 375]    # Let n vary compared to p, e.g. iterate through [200, 500, 750] if you set p = 1000. What truly matters here is the ratio p / n, so if you choose p differently, adjust your choices for n\n",
    "sparsities = [0.75, 0.9, 0.95, 0.99]    # Let sparsity vary for a few choices, e.g. [0.75, 0.9, 0.95, 0.99]\n",
    "SNR = 2     # You can fix SNR at something reasonable like 2 or 5 throughout\n",
    "beta_scale = 5      # Same holds for beta_scale, maybe 5 or 10\n",
    "rng = np.random.default_rng(12345)\n",
    "test_size = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, it will help you tremendously in the interpretation of the results if you **repeat the simulations a few times**, say 5 or 10 times, for each choice of n and sparsity. Here is why you should be careful with your choices: If you chose three values for n and four for sparsity, as well as 5 repeats, then you need to run your simulations for 60 datasets. A setup with 50 datasets took about 2 minutes on a 2017 MacBook, so if it takes hours, you did something wrong :-)\n",
    "\n",
    "It can be good to include intermediate print-outs/clock output throughout the code, e.g. for iteration numbers or cross-validation so you can detect if there is a time sink somewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.          0.          0.          0.         -4.02183271\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      " -2.32477544  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.         -6.56298455  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -9.47148727  0.\n",
      "  0.          0.          0.          0.          0.          6.65848049\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.         -5.19250687\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.         -1.21709386  0.          0.\n",
      "  0.          0.          0.         10.34467971  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  2.14611393  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.         -5.99983431  0.          0.\n",
      "  0.          0.         -0.56179379  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.09080017  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      " -3.66413602  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -5.15429025  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.         -8.79166773  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -5.34653157  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          5.72993953  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          4.83268826  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.         -5.46409375  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.         -6.20688727\n",
      "  0.          0.          0.          0.         13.04324407  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -1.75761919  0.\n",
      "  0.          0.         -2.30464518  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          3.31279916  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      " -1.62278263  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      " -7.68021344  0.        ]\n"
     ]
    }
   ],
   "source": [
    "x, y, beta = simulate_data(n_list[0], p, rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Determine hyperparameters\n",
    "\n",
    "This works as described above the \"Tasks\" section. Depending on the the package you are using you will have to perform some different steps to get the coefficients and the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Comparing the two Lasso models\n",
    "\n",
    "#### Computation of train/test MSE\n",
    "\n",
    "Whenever you use LassoCV or cv.glmnet you can, at the best, access the test error for each fold. To compute both the training and test MSE of the model, you have two options:\n",
    "\n",
    "1. Write your own cross validation loop. Then you can evaluate the training and test error and safe both.\n",
    "2. Instead of only creating a training dataset with the options given in Part 1, Task 1 you can do the following:\n",
    "    - Fix n_test at some larger value, say, n_test = 500 or n_test = 1000\n",
    "    - Instead of simulating n samples you generate now n + n_test and split the dataset. Train on the n samples with cross validation and use the remaining n_test samples for testing. You do not need to adapt the size of the test set to the size of the training dataset as long as you choose it large enough.\n",
    "Option 2 is probably less tedious to implement, but the choice is up to you.\n",
    "\n",
    "When reporting the results for the train/test MSE, **please do not simply report a table of numbers. Find a nice way to visualise the results.**\n",
    "\n",
    "#### Question\n",
    " - How does the MSE of the $λ_{min}$ and $λ_{1se}$ models behave for different n and sparsity levels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.94      0.85       375\n",
      "           1       0.52      0.20      0.29       125\n",
      "\n",
      "    accuracy                           0.75       500\n",
      "   macro avg       0.65      0.57      0.57       500\n",
      "weighted avg       0.71      0.75      0.71       500\n",
      "\n",
      "alpha_min = 6.3885, alpha_1se = 7.8760\n",
      "n = 125, spar = 0.75, alpha_min mse = 5346.0510, alpha_1se mse = 5085.7979\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.98      0.86       375\n",
      "           1       0.67      0.11      0.19       125\n",
      "\n",
      "    accuracy                           0.76       500\n",
      "   macro avg       0.72      0.55      0.53       500\n",
      "weighted avg       0.74      0.76      0.69       500\n",
      "\n",
      "alpha_min = 7.1773, alpha_1se = 8.2521\n",
      "n = 125, spar = 0.75, alpha_min mse = 4026.8574, alpha_1se mse = 3928.8586\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.98      0.86       375\n",
      "           1       0.56      0.08      0.14       125\n",
      "\n",
      "    accuracy                           0.75       500\n",
      "   macro avg       0.66      0.53      0.50       500\n",
      "weighted avg       0.71      0.75      0.68       500\n",
      "\n",
      "alpha_min = 11.3794, alpha_1se = 16.1299\n",
      "n = 125, spar = 0.75, alpha_min mse = 4724.6123, alpha_1se mse = 4452.3380\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.94      0.85       375\n",
      "           1       0.47      0.17      0.25       125\n",
      "\n",
      "    accuracy                           0.74       500\n",
      "   macro avg       0.62      0.55      0.55       500\n",
      "weighted avg       0.70      0.74      0.70       500\n",
      "\n",
      "alpha_min = 6.3461, alpha_1se = 8.9955\n",
      "n = 125, spar = 0.75, alpha_min mse = 4164.9724, alpha_1se mse = 3972.9293\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.98      0.86       375\n",
      "           1       0.62      0.12      0.20       125\n",
      "\n",
      "    accuracy                           0.76       500\n",
      "   macro avg       0.70      0.55      0.53       500\n",
      "weighted avg       0.73      0.76      0.70       500\n",
      "\n",
      "alpha_min = 10.3649, alpha_1se = 15.7538\n",
      "n = 125, spar = 0.75, alpha_min mse = 5100.6286, alpha_1se mse = 4870.3670\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       450\n",
      "           1       0.30      0.36      0.33        50\n",
      "\n",
      "    accuracy                           0.85       500\n",
      "   macro avg       0.61      0.63      0.62       500\n",
      "weighted avg       0.86      0.85      0.86       500\n",
      "\n",
      "alpha_min = 2.0562, alpha_1se = 3.3511\n",
      "n = 125, spar = 0.9, alpha_min mse = 2147.3459, alpha_1se mse = 2026.0565\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.89      0.91       450\n",
      "           1       0.32      0.46      0.38        50\n",
      "\n",
      "    accuracy                           0.85       500\n",
      "   macro avg       0.63      0.68      0.65       500\n",
      "weighted avg       0.88      0.85      0.86       500\n",
      "\n",
      "alpha_min = 2.0871, alpha_1se = 3.9109\n",
      "n = 125, spar = 0.9, alpha_min mse = 1849.5111, alpha_1se mse = 1679.9088\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.83      0.88       450\n",
      "           1       0.23      0.46      0.31        50\n",
      "\n",
      "    accuracy                           0.80       500\n",
      "   macro avg       0.58      0.65      0.60       500\n",
      "weighted avg       0.86      0.80      0.82       500\n",
      "\n",
      "alpha_min = 0.9831, alpha_1se = 1.4943\n",
      "n = 125, spar = 0.9, alpha_min mse = 1723.4021, alpha_1se mse = 1638.3570\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.94       450\n",
      "           1       0.45      0.20      0.28        50\n",
      "\n",
      "    accuracy                           0.90       500\n",
      "   macro avg       0.69      0.59      0.61       500\n",
      "weighted avg       0.87      0.90      0.88       500\n",
      "\n",
      "alpha_min = 6.1896, alpha_1se = 6.6369\n",
      "n = 125, spar = 0.9, alpha_min mse = 1502.7712, alpha_1se mse = 1494.9720\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.91       450\n",
      "           1       0.29      0.38      0.33        50\n",
      "\n",
      "    accuracy                           0.84       500\n",
      "   macro avg       0.61      0.64      0.62       500\n",
      "weighted avg       0.86      0.84      0.85       500\n",
      "\n",
      "alpha_min = 2.0653, alpha_1se = 2.7302\n",
      "n = 125, spar = 0.9, alpha_min mse = 1667.4781, alpha_1se mse = 1612.5467\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.91      0.94       474\n",
      "           1       0.29      0.65      0.40        26\n",
      "\n",
      "    accuracy                           0.90       500\n",
      "   macro avg       0.63      0.78      0.67       500\n",
      "weighted avg       0.94      0.90      0.92       500\n",
      "\n",
      "alpha_min = 1.5173, alpha_1se = 2.8431\n",
      "n = 125, spar = 0.95, alpha_min mse = 1257.8334, alpha_1se mse = 1157.8212\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.91      0.94       474\n",
      "           1       0.27      0.62      0.37        26\n",
      "\n",
      "    accuracy                           0.89       500\n",
      "   macro avg       0.62      0.76      0.66       500\n",
      "weighted avg       0.94      0.89      0.91       500\n",
      "\n",
      "alpha_min = 1.4270, alpha_1se = 3.7901\n",
      "n = 125, spar = 0.95, alpha_min mse = 1063.5441, alpha_1se mse = 881.0626\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.80      0.89       474\n",
      "           1       0.18      0.77      0.29        26\n",
      "\n",
      "    accuracy                           0.80       500\n",
      "   macro avg       0.58      0.79      0.59       500\n",
      "weighted avg       0.94      0.80      0.85       500\n",
      "\n",
      "alpha_min = 0.1988, alpha_1se = 0.6071\n",
      "n = 125, spar = 0.95, alpha_min mse = 1336.8964, alpha_1se mse = 1283.0098\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96       474\n",
      "           1       0.34      0.46      0.39        26\n",
      "\n",
      "    accuracy                           0.93       500\n",
      "   macro avg       0.66      0.71      0.68       500\n",
      "weighted avg       0.94      0.93      0.93       500\n",
      "\n",
      "alpha_min = 2.5122, alpha_1se = 3.3210\n",
      "n = 125, spar = 0.95, alpha_min mse = 852.8517, alpha_1se mse = 821.9997\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.91      0.94       474\n",
      "           1       0.28      0.62      0.39        26\n",
      "\n",
      "    accuracy                           0.90       500\n",
      "   macro avg       0.63      0.76      0.66       500\n",
      "weighted avg       0.94      0.90      0.92       500\n",
      "\n",
      "alpha_min = 1.1163, alpha_1se = 1.3762\n",
      "n = 125, spar = 0.95, alpha_min mse = 652.3127, alpha_1se mse = 636.6211\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       494\n",
      "           1       0.24      1.00      0.39         6\n",
      "\n",
      "    accuracy                           0.96       500\n",
      "   macro avg       0.62      0.98      0.68       500\n",
      "weighted avg       0.99      0.96      0.97       500\n",
      "\n",
      "alpha_min = 0.5467, alpha_1se = 0.6740\n",
      "n = 125, spar = 0.99, alpha_min mse = 554.6765, alpha_1se mse = 551.4791\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       494\n",
      "           1       0.57      0.67      0.62         6\n",
      "\n",
      "    accuracy                           0.99       500\n",
      "   macro avg       0.78      0.83      0.81       500\n",
      "weighted avg       0.99      0.99      0.99       500\n",
      "\n",
      "alpha_min = 1.2696, alpha_1se = 1.5652\n",
      "n = 125, spar = 0.99, alpha_min mse = 258.5868, alpha_1se mse = 247.3216\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98       494\n",
      "           1       0.25      0.83      0.38         6\n",
      "\n",
      "    accuracy                           0.97       500\n",
      "   macro avg       0.62      0.90      0.68       500\n",
      "weighted avg       0.99      0.97      0.98       500\n",
      "\n",
      "alpha_min = 0.6995, alpha_1se = 0.8042\n",
      "n = 125, spar = 0.99, alpha_min mse = 374.0407, alpha_1se mse = 373.3801\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98       494\n",
      "           1       0.23      0.83      0.36         6\n",
      "\n",
      "    accuracy                           0.96       500\n",
      "   macro avg       0.61      0.90      0.67       500\n",
      "weighted avg       0.99      0.96      0.97       500\n",
      "\n",
      "alpha_min = 0.6997, alpha_1se = 0.8045\n",
      "n = 125, spar = 0.99, alpha_min mse = 185.7786, alpha_1se mse = 181.8799\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.85      0.92       494\n",
      "           1       0.08      1.00      0.14         6\n",
      "\n",
      "    accuracy                           0.85       500\n",
      "   macro avg       0.54      0.93      0.53       500\n",
      "weighted avg       0.99      0.85      0.91       500\n",
      "\n",
      "alpha_min = 0.3899, alpha_1se = 0.8400\n",
      "n = 125, spar = 0.99, alpha_min mse = 392.4304, alpha_1se mse = 372.4002\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.93      0.85       375\n",
      "           1       0.55      0.25      0.34       125\n",
      "\n",
      "    accuracy                           0.76       500\n",
      "   macro avg       0.67      0.59      0.60       500\n",
      "weighted avg       0.73      0.76      0.73       500\n",
      "\n",
      "alpha_min = 4.7337, alpha_1se = 6.2577\n",
      "n = 250, spar = 0.75, alpha_min mse = 4162.1946, alpha_1se mse = 4003.3553\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.79      0.80       375\n",
      "           1       0.43      0.47      0.45       125\n",
      "\n",
      "    accuracy                           0.71       500\n",
      "   macro avg       0.62      0.63      0.63       500\n",
      "weighted avg       0.72      0.71      0.72       500\n",
      "\n",
      "alpha_min = 1.7618, alpha_1se = 3.0789\n",
      "n = 250, spar = 0.75, alpha_min mse = 5968.0166, alpha_1se mse = 5439.0521\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.81      0.82       375\n",
      "           1       0.48      0.53      0.50       125\n",
      "\n",
      "    accuracy                           0.74       500\n",
      "   macro avg       0.66      0.67      0.66       500\n",
      "weighted avg       0.75      0.74      0.74       500\n",
      "\n",
      "alpha_min = 1.5147, alpha_1se = 1.8674\n",
      "n = 250, spar = 0.75, alpha_min mse = 6416.6408, alpha_1se mse = 6190.1170\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.90      0.85       375\n",
      "           1       0.56      0.38      0.45       125\n",
      "\n",
      "    accuracy                           0.77       500\n",
      "   macro avg       0.69      0.64      0.65       500\n",
      "weighted avg       0.75      0.77      0.75       500\n",
      "\n",
      "alpha_min = 3.1522, alpha_1se = 3.6242\n",
      "n = 250, spar = 0.75, alpha_min mse = 4554.4985, alpha_1se mse = 4498.9706\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.81      0.83       375\n",
      "           1       0.51      0.61      0.56       125\n",
      "\n",
      "    accuracy                           0.76       500\n",
      "   macro avg       0.69      0.71      0.70       500\n",
      "weighted avg       0.77      0.76      0.76       500\n",
      "\n",
      "alpha_min = 1.5140, alpha_1se = 1.8666\n",
      "n = 250, spar = 0.75, alpha_min mse = 5016.6776, alpha_1se mse = 4875.7657\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.86      0.90       450\n",
      "           1       0.31      0.58      0.41        50\n",
      "\n",
      "    accuracy                           0.83       500\n",
      "   macro avg       0.63      0.72      0.65       500\n",
      "weighted avg       0.88      0.83      0.85       500\n",
      "\n",
      "alpha_min = 1.4348, alpha_1se = 2.0337\n",
      "n = 250, spar = 0.9, alpha_min mse = 2058.7524, alpha_1se mse = 1931.6506\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.84      0.89       450\n",
      "           1       0.32      0.70      0.44        50\n",
      "\n",
      "    accuracy                           0.82       500\n",
      "   macro avg       0.64      0.77      0.67       500\n",
      "weighted avg       0.90      0.82      0.85       500\n",
      "\n",
      "alpha_min = 1.2073, alpha_1se = 1.7114\n",
      "n = 250, spar = 0.9, alpha_min mse = 2932.9953, alpha_1se mse = 2819.8811\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.84      0.90       450\n",
      "           1       0.35      0.74      0.47        50\n",
      "\n",
      "    accuracy                           0.83       500\n",
      "   macro avg       0.66      0.79      0.69       500\n",
      "weighted avg       0.90      0.83      0.86       500\n",
      "\n",
      "alpha_min = 1.4417, alpha_1se = 1.9058\n",
      "n = 250, spar = 0.9, alpha_min mse = 1774.1945, alpha_1se mse = 1674.1856\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.90      0.92       450\n",
      "           1       0.36      0.52      0.42        50\n",
      "\n",
      "    accuracy                           0.86       500\n",
      "   macro avg       0.65      0.71      0.67       500\n",
      "weighted avg       0.89      0.86      0.87       500\n",
      "\n",
      "alpha_min = 1.9190, alpha_1se = 2.0577\n",
      "n = 250, spar = 0.9, alpha_min mse = 1823.1222, alpha_1se mse = 1805.9790\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.87      0.91       450\n",
      "           1       0.34      0.62      0.44        50\n",
      "\n",
      "    accuracy                           0.84       500\n",
      "   macro avg       0.65      0.74      0.67       500\n",
      "weighted avg       0.89      0.84      0.86       500\n",
      "\n",
      "alpha_min = 1.2614, alpha_1se = 1.7880\n",
      "n = 250, spar = 0.9, alpha_min mse = 2449.4196, alpha_1se mse = 2352.7967\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.88      0.93       474\n",
      "           1       0.28      0.85      0.42        26\n",
      "\n",
      "    accuracy                           0.88       500\n",
      "   macro avg       0.64      0.86      0.68       500\n",
      "weighted avg       0.95      0.88      0.91       500\n",
      "\n",
      "alpha_min = 0.9461, alpha_1se = 1.2507\n",
      "n = 250, spar = 0.95, alpha_min mse = 1725.2647, alpha_1se mse = 1695.9016\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.89      0.93       474\n",
      "           1       0.23      0.62      0.33        26\n",
      "\n",
      "    accuracy                           0.87       500\n",
      "   macro avg       0.60      0.75      0.63       500\n",
      "weighted avg       0.94      0.87      0.90       500\n",
      "\n",
      "alpha_min = 1.5149, alpha_1se = 1.7417\n",
      "n = 250, spar = 0.95, alpha_min mse = 1741.9444, alpha_1se mse = 1700.5356\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.91      0.95       474\n",
      "           1       0.33      0.81      0.47        26\n",
      "\n",
      "    accuracy                           0.91       500\n",
      "   macro avg       0.66      0.86      0.71       500\n",
      "weighted avg       0.95      0.91      0.92       500\n",
      "\n",
      "alpha_min = 0.9490, alpha_1se = 1.1699\n",
      "n = 250, spar = 0.95, alpha_min mse = 888.1700, alpha_1se mse = 873.6320\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.82      0.90       474\n",
      "           1       0.19      0.73      0.30        26\n",
      "\n",
      "    accuracy                           0.82       500\n",
      "   macro avg       0.58      0.78      0.60       500\n",
      "weighted avg       0.94      0.82      0.87       500\n",
      "\n",
      "alpha_min = 0.9461, alpha_1se = 1.4380\n",
      "n = 250, spar = 0.95, alpha_min mse = 971.1008, alpha_1se mse = 926.3625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.92      0.95       474\n",
      "           1       0.32      0.73      0.45        26\n",
      "\n",
      "    accuracy                           0.91       500\n",
      "   macro avg       0.65      0.82      0.70       500\n",
      "weighted avg       0.95      0.91      0.92       500\n",
      "\n",
      "alpha_min = 1.6658, alpha_1se = 2.2021\n",
      "n = 250, spar = 0.95, alpha_min mse = 1352.4852, alpha_1se mse = 1275.2361\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       494\n",
      "           1       0.21      1.00      0.35         6\n",
      "\n",
      "    accuracy                           0.96       500\n",
      "   macro avg       0.61      0.98      0.67       500\n",
      "weighted avg       0.99      0.96      0.97       500\n",
      "\n",
      "alpha_min = 0.7717, alpha_1se = 0.9514\n",
      "n = 250, spar = 0.99, alpha_min mse = 208.9223, alpha_1se mse = 200.0293\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98       494\n",
      "           1       0.24      0.67      0.35         6\n",
      "\n",
      "    accuracy                           0.97       500\n",
      "   macro avg       0.62      0.82      0.67       500\n",
      "weighted avg       0.99      0.97      0.98       500\n",
      "\n",
      "alpha_min = 1.2315, alpha_1se = 1.4160\n",
      "n = 250, spar = 0.99, alpha_min mse = 542.0473, alpha_1se mse = 529.9259\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96       494\n",
      "           1       0.14      1.00      0.24         6\n",
      "\n",
      "    accuracy                           0.92       500\n",
      "   macro avg       0.57      0.96      0.60       500\n",
      "weighted avg       0.99      0.92      0.95       500\n",
      "\n",
      "alpha_min = 0.7117, alpha_1se = 0.8183\n",
      "n = 250, spar = 0.99, alpha_min mse = 274.3715, alpha_1se mse = 268.7629\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       494\n",
      "           1       0.67      0.67      0.67         6\n",
      "\n",
      "    accuracy                           0.99       500\n",
      "   macro avg       0.83      0.83      0.83       500\n",
      "weighted avg       0.99      0.99      0.99       500\n",
      "\n",
      "alpha_min = 0.7975, alpha_1se = 0.9170\n",
      "n = 250, spar = 0.99, alpha_min mse = 337.1692, alpha_1se mse = 333.7605\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       494\n",
      "           1       0.50      0.50      0.50         6\n",
      "\n",
      "    accuracy                           0.99       500\n",
      "   macro avg       0.75      0.75      0.75       500\n",
      "weighted avg       0.99      0.99      0.99       500\n",
      "\n",
      "alpha_min = 1.0676, alpha_1se = 1.1448\n",
      "n = 250, spar = 0.99, alpha_min mse = 184.0841, alpha_1se mse = 181.7428\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.77      0.82       375\n",
      "           1       0.49      0.66      0.56       125\n",
      "\n",
      "    accuracy                           0.74       500\n",
      "   macro avg       0.68      0.72      0.69       500\n",
      "weighted avg       0.78      0.74      0.75       500\n",
      "\n",
      "alpha_min = 1.4259, alpha_1se = 1.7579\n",
      "n = 375, spar = 0.75, alpha_min mse = 6254.3546, alpha_1se mse = 6097.6913\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.62      0.74       375\n",
      "           1       0.41      0.81      0.55       125\n",
      "\n",
      "    accuracy                           0.67       500\n",
      "   macro avg       0.66      0.71      0.64       500\n",
      "weighted avg       0.78      0.67      0.69       500\n",
      "\n",
      "alpha_min = 0.6616, alpha_1se = 0.9378\n",
      "n = 375, spar = 0.75, alpha_min mse = 5500.0500, alpha_1se mse = 5110.3840\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.75      0.81       375\n",
      "           1       0.47      0.68      0.56       125\n",
      "\n",
      "    accuracy                           0.73       500\n",
      "   macro avg       0.68      0.71      0.68       500\n",
      "weighted avg       0.78      0.73      0.75       500\n",
      "\n",
      "alpha_min = 1.1761, alpha_1se = 1.4499\n",
      "n = 375, spar = 0.75, alpha_min mse = 5810.2522, alpha_1se mse = 5567.6049\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.71      0.79       375\n",
      "           1       0.45      0.71      0.55       125\n",
      "\n",
      "    accuracy                           0.71       500\n",
      "   macro avg       0.67      0.71      0.67       500\n",
      "weighted avg       0.77      0.71      0.73       500\n",
      "\n",
      "alpha_min = 0.9916, alpha_1se = 1.6161\n",
      "n = 375, spar = 0.75, alpha_min mse = 5241.7503, alpha_1se mse = 4914.4384\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.56      0.68       375\n",
      "           1       0.37      0.77      0.50       125\n",
      "\n",
      "    accuracy                           0.61       500\n",
      "   macro avg       0.62      0.66      0.59       500\n",
      "weighted avg       0.75      0.61      0.64       500\n",
      "\n",
      "alpha_min = 0.5996, alpha_1se = 0.7926\n",
      "n = 375, spar = 0.75, alpha_min mse = 6301.9107, alpha_1se mse = 6116.7233\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.75      0.84       450\n",
      "           1       0.26      0.80      0.39        50\n",
      "\n",
      "    accuracy                           0.75       500\n",
      "   macro avg       0.62      0.77      0.62       500\n",
      "weighted avg       0.90      0.75      0.80       500\n",
      "\n",
      "alpha_min = 0.9017, alpha_1se = 1.1920\n",
      "n = 375, spar = 0.9, alpha_min mse = 3675.4387, alpha_1se mse = 3560.7277\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.76      0.85       450\n",
      "           1       0.26      0.76      0.39        50\n",
      "\n",
      "    accuracy                           0.76       500\n",
      "   macro avg       0.61      0.76      0.62       500\n",
      "weighted avg       0.90      0.76      0.80       500\n",
      "\n",
      "alpha_min = 0.6828, alpha_1se = 0.9678\n",
      "n = 375, spar = 0.9, alpha_min mse = 2222.6795, alpha_1se mse = 2129.4041\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.81      0.88       450\n",
      "           1       0.31      0.78      0.45        50\n",
      "\n",
      "    accuracy                           0.81       500\n",
      "   macro avg       0.64      0.80      0.67       500\n",
      "weighted avg       0.91      0.81      0.84       500\n",
      "\n",
      "alpha_min = 0.8978, alpha_1se = 1.0323\n",
      "n = 375, spar = 0.9, alpha_min mse = 2950.5223, alpha_1se mse = 2869.7588\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.85      0.90       450\n",
      "           1       0.34      0.68      0.45        50\n",
      "\n",
      "    accuracy                           0.84       500\n",
      "   macro avg       0.65      0.77      0.68       500\n",
      "weighted avg       0.90      0.84      0.86       500\n",
      "\n",
      "alpha_min = 1.0095, alpha_1se = 1.1607\n",
      "n = 375, spar = 0.9, alpha_min mse = 3003.7666, alpha_1se mse = 2966.0230\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.78      0.87       450\n",
      "           1       0.30      0.84      0.44        50\n",
      "\n",
      "    accuracy                           0.79       500\n",
      "   macro avg       0.64      0.81      0.66       500\n",
      "weighted avg       0.91      0.79      0.83       500\n",
      "\n",
      "alpha_min = 0.8225, alpha_1se = 1.0140\n",
      "n = 375, spar = 0.9, alpha_min mse = 1681.1445, alpha_1se mse = 1614.2246\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.88      0.93       474\n",
      "           1       0.26      0.77      0.39        26\n",
      "\n",
      "    accuracy                           0.88       500\n",
      "   macro avg       0.62      0.83      0.66       500\n",
      "weighted avg       0.95      0.88      0.90       500\n",
      "\n",
      "alpha_min = 0.7735, alpha_1se = 0.8893\n",
      "n = 375, spar = 0.95, alpha_min mse = 875.4102, alpha_1se mse = 861.8958\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.89      0.93       474\n",
      "           1       0.24      0.62      0.34        26\n",
      "\n",
      "    accuracy                           0.88       500\n",
      "   macro avg       0.61      0.75      0.64       500\n",
      "weighted avg       0.94      0.88      0.90       500\n",
      "\n",
      "alpha_min = 1.0511, alpha_1se = 1.2958\n",
      "n = 375, spar = 0.95, alpha_min mse = 1570.7437, alpha_1se mse = 1548.0888\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.89      0.93       474\n",
      "           1       0.27      0.77      0.40        26\n",
      "\n",
      "    accuracy                           0.88       500\n",
      "   macro avg       0.63      0.83      0.67       500\n",
      "weighted avg       0.95      0.88      0.91       500\n",
      "\n",
      "alpha_min = 0.7434, alpha_1se = 0.9827\n",
      "n = 375, spar = 0.95, alpha_min mse = 1222.3897, alpha_1se mse = 1196.5749\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.87      0.93       474\n",
      "           1       0.27      0.88      0.42        26\n",
      "\n",
      "    accuracy                           0.87       500\n",
      "   macro avg       0.63      0.88      0.67       500\n",
      "weighted avg       0.96      0.87      0.90       500\n",
      "\n",
      "alpha_min = 0.8158, alpha_1se = 1.0057\n",
      "n = 375, spar = 0.95, alpha_min mse = 864.5396, alpha_1se mse = 840.2328\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95       474\n",
      "           1       0.34      0.92      0.49        26\n",
      "\n",
      "    accuracy                           0.90       500\n",
      "   macro avg       0.67      0.91      0.72       500\n",
      "weighted avg       0.96      0.90      0.92       500\n",
      "\n",
      "alpha_min = 0.9549, alpha_1se = 1.0979\n",
      "n = 375, spar = 0.95, alpha_min mse = 1643.4296, alpha_1se mse = 1616.6962\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96       494\n",
      "           1       0.13      1.00      0.23         6\n",
      "\n",
      "    accuracy                           0.92       500\n",
      "   macro avg       0.56      0.96      0.59       500\n",
      "weighted avg       0.99      0.92      0.95       500\n",
      "\n",
      "alpha_min = 0.4245, alpha_1se = 0.4881\n",
      "n = 375, spar = 0.99, alpha_min mse = 295.7468, alpha_1se mse = 292.5731\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99       494\n",
      "           1       0.32      1.00      0.48         6\n",
      "\n",
      "    accuracy                           0.97       500\n",
      "   macro avg       0.66      0.99      0.73       500\n",
      "weighted avg       0.99      0.97      0.98       500\n",
      "\n",
      "alpha_min = 0.4112, alpha_1se = 0.4728\n",
      "n = 375, spar = 0.99, alpha_min mse = 234.2688, alpha_1se mse = 233.0274\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98       494\n",
      "           1       0.20      1.00      0.33         6\n",
      "\n",
      "    accuracy                           0.95       500\n",
      "   macro avg       0.60      0.98      0.65       500\n",
      "weighted avg       0.99      0.95      0.97       500\n",
      "\n",
      "alpha_min = 0.5779, alpha_1se = 0.7125\n",
      "n = 375, spar = 0.99, alpha_min mse = 191.4004, alpha_1se mse = 184.3814\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97       494\n",
      "           1       0.16      0.83      0.27         6\n",
      "\n",
      "    accuracy                           0.95       500\n",
      "   macro avg       0.58      0.89      0.62       500\n",
      "weighted avg       0.99      0.95      0.96       500\n",
      "\n",
      "alpha_min = 0.3336, alpha_1se = 0.4113\n",
      "n = 375, spar = 0.99, alpha_min mse = 127.3498, alpha_1se mse = 125.1180\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       494\n",
      "           1       0.50      0.83      0.62         6\n",
      "\n",
      "    accuracy                           0.99       500\n",
      "   macro avg       0.75      0.91      0.81       500\n",
      "weighted avg       0.99      0.99      0.99       500\n",
      "\n",
      "alpha_min = 0.8962, alpha_1se = 1.0304\n",
      "n = 375, spar = 0.99, alpha_min mse = 240.8551, alpha_1se mse = 235.7886\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "train_mse_min = np.empty(shape=(len(n_list), len(sparsities), 5))\n",
    "train_mse_ose = np.empty(shape=(len(n_list), len(sparsities), 5))\n",
    "\n",
    "test_mse_min = np.empty(shape=(len(n_list), len(sparsities), 5))\n",
    "test_mse_ose = np.empty(shape=(len(n_list), len(sparsities), 5))\n",
    "\n",
    "for i, n in enumerate(n_list):\n",
    "    for j, sparsity in enumerate(sparsities):\n",
    "        for k in range(5):\n",
    "\n",
    "            x, y, beta = simulate_data(n, p, rng, sparsity=sparsity)\n",
    "            lasso = LassoCV(cv=5).fit(x, y)\n",
    "            mse_mat = lasso.mse_path_\n",
    "            n_folds = mse_mat.shape[0]\n",
    "            cv_mean = np.mean(mse_mat, axis=1)\n",
    "            cv_std = np.std(mse_mat, axis=1)\n",
    "            idx_min_mean = np.argmin(cv_mean)\n",
    "            idx_alpha = np.where(\n",
    "                (cv_mean <= cv_mean[idx_min_mean] + cv_std[idx_min_mean] / np.sqrt(n_folds)) &\n",
    "                (cv_mean >= cv_mean[idx_min_mean])\n",
    "            )[0][0]\n",
    "            alpha_1se = lasso.alphas_[idx_alpha]\n",
    "            \n",
    "            lasso_min = Lasso(alpha=lasso.alpha_).fit(x, y)\n",
    "            lasso_1se = Lasso(alpha=alpha_1se).fit(x, y)\n",
    "\n",
    "            train_min = mse(y, lasso_min.predict(x))\n",
    "            train_1se = mse(y, lasso_1se.predict(x))\n",
    "\n",
    "            beta = np.where(beta!=0, 1, 0)\n",
    "            beta_min = np.where(lasso_min.coef_!=0, 1, 0)\n",
    "            beta_1se = np.where(lasso_1se.coef_!=0, 1, 0)\n",
    "\n",
    "            metrics = classification_report(beta, beta_min)\n",
    "            print(metrics) # TODO fixa detta\n",
    "\n",
    "            x_test, y_test, test_beta = simulate_data(test_size, p, rng, sparsity=sparsity)\n",
    "\n",
    "            train_mse_min[i,j,k] = train_min\n",
    "            train_mse_ose[i,j,k] = train_1se\n",
    "\n",
    "            test_min = mse(y_test, lasso_min.predict(x_test))\n",
    "            test_1se = mse(y_test, lasso_1se.predict(x_test))\n",
    "\n",
    "            test_mse_min[i,j,k] = test_min\n",
    "            test_mse_ose[i,j,k] = test_1se\n",
    "            \n",
    "            \n",
    "            print(f'alpha_min = {lasso.alpha_:.4f}, alpha_1se = {alpha_1se:.4f}')\n",
    "            print(f'n = {n}, spar = {sparsity}, alpha_min mse = {test_min:.4f}, alpha_1se mse = {test_1se:.4f}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98\n",
      "500\n",
      "[ 0.          0.          0.          0.          0.         -4.02183271\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      " -2.32477544  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.         -6.56298455  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -9.47148727  0.\n",
      "  0.          0.          0.          0.          0.          6.65848049\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.         -5.19250687\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.         -1.21709386  0.          0.\n",
      "  0.          0.          0.         10.34467971  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  2.14611393  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.         -5.99983431  0.          0.\n",
      "  0.          0.         -0.56179379  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.09080017  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      " -3.66413602  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -5.15429025  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.         -8.79166773  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -5.34653157  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          5.72993953  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          4.83268826  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.         -5.46409375  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.         -6.20688727\n",
      "  0.          0.          0.          0.         13.04324407  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -1.75761919  0.\n",
      "  0.          0.         -2.30464518  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          3.31279916  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      " -1.62278263  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      " -7.68021344  0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(len(lasso_1se.coef_[lasso_1se.coef_==0])/len(lasso_1se.coef_))\n",
    "print(len(lasso_1se.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4513.96837747 2044.84842464 1138.49600798  270.61093617]\n",
      " [5111.41253679 2348.88016697 1304.61398011  278.21271288]\n",
      " [5805.57687462 2511.41479441 1165.49493588  241.21926317]]\n",
      "[[4225.05370968 1907.48291172 1109.62849229  261.81744347]\n",
      " [4858.35741978 2251.41007387 1265.97024284  270.46555756]\n",
      " [5573.6218323  2433.02639384 1138.3757905   236.0264413 ]]\n",
      "-----------------------------------\n",
      "[[1447.66268621  288.69879851  105.44463228   23.23783414]\n",
      " [ 787.11927211  202.74547054  128.47130524   41.10305801]\n",
      " [ 522.93498514  253.27108625  109.06382245   36.13977153]]\n",
      "[[2100.86215432  434.13964479  146.63964874   31.32889619]\n",
      " [1164.79098951  282.77062122  156.75338765   47.05696264]\n",
      " [ 668.03596838  308.96394726  127.94587218   39.83460947]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x2401ddf8460>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAADwCAYAAAD/9/QXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAYpklEQVR4nO3df6we1Z3f8fcH4wAlsPwwBscmgUpuJEAKPyzHEVJFSggOjWqkhsqRGtwIyYISKVEjdU0i7WorWaL9I2oRC8hNUOw2C7WUEKzUQBw3EUnFL0MBYwzFAQSWLbzALuAk6+B7P/1jzrM8vTzPvWOeuc+dO/t5SUfPPGfOzDke3/v18ZkzZ2SbiIjohuPmugEREdGcBPWIiA5JUI+I6JAE9YiIDklQj4jokAT1iIgOOX6uGxAR0QZXf/5kv/X2RK2yTz575CHbq2e5SR9JgnpEBPDm2xM89tCyWmUXLvntolluzkeWoB4RAYCZ8ORcN2JkCeoREYCBSeb/E/YjBXVJZwD/AzgPeBX4V7b/ZkC5V4H3gAngqO0Vo9QbEdE0Y953vTH1Nht19ssGYKft5cDO8n2Yz9u+OAE9ItpqEtdKbTZqUF8DbC7bm4FrRzxfRMScMDCBa6U2GzWon237IED5XDyknIGfS3pS0voR64yImBVd6KnPOKYu6RfAOQN2ffcY6rnc9gFJi4Edkl6w/fCQ+tYD6wGOP+n4y04779RjqKa7fvfX/2ium9Aax/9h/s9QaMzv/jDXLWiFv+N3/NFHNMo5DEx0YCnyGYO67S8M2yfpDUlLbB+UtAQ4NOQcB8rnIUn3ASuBgUHd9iZgE8BZF5zpf/nfrpn5T/EPwON3XTLXTWiNM3cfnusmtMcTz811C1rhsclfjHwOY95veS+8jlGHX7YB68r2OuD+qQUknSzplN428EUgP4kR0S6GiZqpzUYN6rcCV0l6CbiqfEfSJyRtL2XOBn4j6RngceB/2n5wxHojIhpVzVOvl9pspHnqtt8CrhyQfwC4pmy/DHxmlHoiImafmGCkYflWyBOlERGUnnrLh1bqSFCPiCjSU4+I6AgD73v+v2IiQT0igt4TpempR0R0ghETHXgZXIJ6REQx6fTUIyI6IcMvERGdIiZyozQiohsMvM+CuW7GyBLUIyIAOz31iIhOmcyYekREN1Q3StNTj4joiAy/RER0RrVMQG6URkR0QleeKJ3/f4KIiIZM+rhaqQ5Jr0raLelpSbtK3hmSdkh6qXye3lf+Fkn7JL0o6eq+/MvKefZJuk3StHdzE9QjIvjgRmmddAw+b/ti2yvK9w3ATtvLgZ3lO5IuANYCFwKrgTsk9caC7gTWA8tLWj1dhQnqERGU4RfXSyNYA2wu25uBa/vy77V9xPYrwD5gpaQlwKm2H7FtYEvfMQMlqEdEFJMcVysBiyTt6kvrB5zOwM8lPdm3/2zbBwHK5+KSvxR4ve/Y/SVvadmemj9UbpRGRFA9UXoMs1/e7BtSGeZy2wckLQZ2SHphmrKDuv+eJn+oBPWICMqYeoPz1G0fKJ+HJN0HrATekLTE9sEytHKoFN8PnNt3+DLgQMlfNiB/qAy/REQUTd0olXSypFN628AXgeeAbcC6UmwdcH/Z3gaslXSCpPOpbog+XoZo3pO0qsx6ub7vmIHSU4+IoLpR2uBLMs4G7iuzD48H/sr2g5KeALZKugF4DbgOwPYeSVuB54GjwM22J8q5bgJ+CJwEPFDSUAnqERFFUw8f2X4Z+MyA/LeAK4ccsxHYOCB/F3BR3boT1CMiqMbU6z5Y1GaN/AkkrS5PQe2TtGHAfpUnofZJelbSpU3UGxHRFFPNfqmT2mzknnp56ukvgauo7tQ+IWmb7ef7in2JD56G+izVE1KfHbXuiIgmdeEdpU301FcC+2y/bPuPwL1UT0f1WwNsceVR4LQynSciohVsNbr2y1xponXDnoQ61jIASFrfe0rr7/7mSAPNi4ioZ8LH1Upt1kTr6jzxVPupKNubbK+wveLE008YuXEREXWY6nV2dVKbNTH7ZdiTUMdaJiJizhjx/mS7b4LW0URP/QlguaTzJX2MavnIbVPKbAOuL7NgVgHv9Ba1iYhoi1lYenfsRu6p2z4q6RvAQ8AC4O7ydNSNZf9dwHbgGqrlJH8PfH3UeiMimtTwE6VzppGHj2xvpwrc/Xl39W0buLmJuiIiZstky3vhdeSJ0ogIwGbUF2C0QoJ6RESR4ZeIiI7oLRMw3yWoR0TQW9ArPfWIiI5Q65cAqCNBPSKiaPvTonUkqEdEkNkvERGdYsTRDiwTkKAeEVFk+CUioiMy+yUiomMy+yUioiucBb0iIjqj95KM+S5BPSKCKqgfnczwS0REZ2T4JSKiI/KSjIiIjsmYekREVzjDLxERnZGHjyIiOqRa+2X+z36Z/3+CiIiG2KqV6pC0QNL/kfSz8v0MSTskvVQ+T+8re4ukfZJelHR1X/5lknaXfbdJmrHyBPWIiGIS1Uo1fRPY2/d9A7DT9nJgZ/mOpAuAtcCFwGrgDkm95SLvBNYDy0taPVOlCeoREVTrqU+WpQJmSjORtAz458D3+7LXAJvL9mbg2r78e20fsf0KsA9YKWkJcKrtR2wb2NJ3zFAZU4+IKOoOrQCLJO3q+77J9qa+7/8Z+PfAKX15Z9s+WNXjg5IWl/ylwKN95faXvPfL9tT8aTUS1CWtBv4LsAD4vu1bp+y/ArgfeKVk/cT2f2ii7oiIZoiJ+jdK37S9YuBZpC8Dh2w/WWLfzBV/mKfJn9bIQb2M/fwlcBXVvyRPSNpm+/kpRX9t+8uj1hcRMRsanNJ4OfAvJF0DnAicKum/A29IWlJ66UuAQ6X8fuDcvuOXAQdK/rIB+dNqYkx9JbDP9su2/wjcSzVGFBExf7gaV6+Tpj2NfYvtZbbPo7oB+r9s/2tgG7CuFFtHNXpByV8r6QRJ51PdEH28DNW8J2lVmfVyfd8xQzUR1JcCr/d9Hzbu8zlJz0h6QNKFDdQbEdGohme/THUrcJWkl6hGNm4FsL0H2Ao8DzwI3Gx7ohxzE9XN1n3Ab4EHZqqkiTH1OuM+TwGfsn24/Jfkp1T/Gn34ZNJ6qik8LPrEQr542nMNNHH++9+nXjrXTWiNyZNyf78n09eaY47pRmm9c9q/An5Vtt8CrhxSbiOwcUD+LuCiY6mziZ+JYeNBf8/2u7YPl+3twEJJiwadzPYm2ytsr/iTM/LLGxHjUm86Y9uXEmgiqD8BLJd0vqSPUY0hbesvIOmc3pNQklaWet9qoO6IiMZMTqpWarORu8K2j0r6BvAQ1ZTGu23vkXRj2X8X8BXgJklHgT8Aa8tk+oiIVqhugrY7YNfRyPhGGVLZPiXvrr7t24Hbm6grImK2tH1opY4MWkdEFF0YP0hQj4goMvwSEdERpv6yum2WoB4RAXmdXURE52RMPSKiOzL8EhHRIZn9EhHREbOx9stcSFCPiIBq6d2WLwFQR4J6RERPhl8iIroi89QjIrolPfWIiI7IKo0RER2TnnpERIekpx4R0SHpqUdEdIRJTz0iokuyTEBERJckqEdEdEiGXyIiOsKgybluxOgS1CMiAFB66hERnZIx9YiIDklQj4jokA4E9eOaOImkuyUdkvTckP2SdJukfZKelXRpE/VGRDTGoEnVSm3WSFAHfgisnmb/l4DlJa0H7myo3oiI5rhmmoGkEyU9LukZSXsk/UXJP0PSDkkvlc/T+465pXR8X5R0dV/+ZZJ2l323SZr2X5VGgrrth4G3pymyBtjiyqPAaZKWNFF3REQLHQH+me3PABcDqyWtAjYAO20vB3aW70i6AFgLXEjVQb5D0oJyrjupOsO9jvF0HejGeuozWQq83vd9f8n7EEnrJe2StOudt4+OpXEREQByvTST0oE9XL4uLMlUHdzNJX8zcG3ZXgPca/uI7VeAfcDK0vk91fYjtg1s6TtmoHEF9UH/XRh4aWxvsr3C9oo/OSP3cSNijKx6CRb1Op8lrZ96KkkLJD0NHAJ22H4MONv2QYDyubgUH9bxXVq2p+YPNa6ouR84t+/7MuDAmOqOiJhZzfHy4k3bK6Y9nT0BXCzpNOA+SRdNU3xYx7d2h7hnXD31bcD1ZRbMKuCd3r9WERFtocl66VjY/lvgV1Rj4W/07ieWz0Ol2LCO7/6yPTV/qKamNN4DPAJ8WtJ+STdIulHSjaXIduBlqnGi/wr82ybqjYhoVHOzX84qPXQknQR8AXiBqoO7rhRbB9xftrcBayWdIOl8qhuij5fO73uSVpVZL9f3HTNQI8Mvtr86w34DNzdRV0TErGnu4aMlwOYyg+U4YKvtn0l6BNgq6QbgNeA6ANt7JG0FngeOAjeX4RuAm6imjZ8EPFDSULkTGRFB/Zktddh+FrhkQP5bwJVDjtkIbByQvwuYbjz+/5OgHhHRk1UaIyI6pANrvySoR0QUeUlGRERXNDimPpcS1CMiehLUIyI6JEE9IqI7MvwSEdElCeoRER2RG6URER2ToB4R0SEJ6hER3SAy/BIR0S0J6hERHeEsExAR0S3pqUdEdEfG1CMiuiRBPSKiI2q+f7TtEtQjIorcKI2I6JCMqUdEdEmCekRER2RMPSKiO1TSfHdcEyeRdLekQ5KeG7L/CknvSHq6pD9rot6IiEa5ZmqxpnrqPwRuB7ZMU+bXtr/cUH0REY3rwuyXRnrqth8G3m7iXBERc6YDPfVGgnpNn5P0jKQHJF04xnojImZW3nxUJ7XZuG6UPgV8yvZhSdcAPwWWDyooaT2wHuCTS4/n2pMPj6mJ7fbdE+e6Be1x3JGJuW5Ce7jlEWa+6cDlHEtP3fa7tg+X7e3AQkmLhpTdZHuF7RVnnblgHM2LiAC60VMfS1CXdI4kle2Vpd63xlF3RERtGVOvSLoHeAT4tKT9km6QdKOkG0uRrwDPSXoGuA1Ya+f/jRHRIuUlGXXSTCSdK+mXkvZK2iPpmyX/DEk7JL1UPk/vO+YWSfskvSjp6r78yyTtLvtu63WQh2lkTN32V2fYfzvVlMeIiPZqrqt5FPi27acknQI8KWkH8G+AnbZvlbQB2AD8qaQLgLXAhcAngF9I+ie2J4A7qe4zPgpsB1YDDwyreJyzXyIiWqv34ukmxtRtH7T9VNl+D9gLLAXWAJtLsc3AtWV7DXCv7SO2XwH2ASslLQFOtf1IGd3Y0nfMQFkmICKip35PfZGkXX3fN9neNKigpPOAS4DHgLNtH4Qq8EtaXIotpeqJ9+wvee+X7an5QyWoR0QUqn+r703bK2Y8n/Rx4MfAt2y/O81w+KAdniZ/qAy/RERAozdKASQtpAroP7L9k5L9RhlSoXweKvn7gXP7Dl8GHCj5ywbkD5WgHhHR09CUxjJD5QfAXtvf69u1DVhXttcB9/flr5V0gqTzqR7OfLwM1bwnaVU55/V9xwyU4ZeIiKLBB4suB74G7Jb0dMn7DnArsFXSDcBrwHUAtvdI2go8TzVz5uYy8wXgJqpFE0+imvUydOYLJKhHRHygoaBu+zcMX579yiHHbAQ2DsjfBVxUt+4E9YgI+PsFvea7BPWIiJ4E9YiIbhCgyfkf1RPUIyKKDL9ERHTFPFiBsY4E9YiIogvvKE1Qj4joSU89IqI7MqYeEdEVzuyXiIhumf8xPUE9IgI+eEnGfJegHhEBYFdpnktQj4go0lOPiOiQzFOPiOgKA5n9EhHRIfM/pieoR0T0ZEw9IqJLOjD7ZeQXT0s6V9IvJe2VtEfSNweUkaTbJO2T9KykS0etNyKiaXK91GZN9NSPAt+2/ZSkU4AnJe2w/XxfmS9RvR17OfBZ4M7yGRHRCurIMgEj99RtH7T9VNl+D9gLLJ1SbA2wxZVHgdMkLRm17oiIRk3WTC02clDvJ+k84BLgsSm7lgKv933fz4cDf+8c6yXtkrTrr9+aaLJ5ERHTkl0rtVljQV3Sx4EfA9+y/e7U3QMOGXhlbG+yvcL2irPOXNBU8yIipudjSC3WyOwXSQupAvqPbP9kQJH9wLl935cBB5qoOyKiGd1Y+6WJ2S8CfgDstf29IcW2AdeXWTCrgHdsHxy17oiIJmX2S+Vy4GvAbklPl7zvAJ8EsH0XsB24BtgH/B74egP1RkQ0x6CJlkfsGkYO6rZ/w+Ax8/4yBm4eta6IiFnVgeGXPFEaEdEz/2N6gnpERE/bpyvWkaAeEdHTgaDe6MNHERHzlWw0US/NeC7pbkmHJD3Xl3eGpB2SXiqfp/ftu6WsjfWipKv78i+TtLvsu63MNpxWgnpERE/vPaUzpZn9EFg9JW8DsNP2cmBn+Y6kC4C1wIXlmDsk9Z68vBNYzwdrZ00954ckqEdE9DQU1G0/DLw9JXsNsLlsbwau7cu/1/YR269QTf1eWdbHOtX2I2UG4Za+Y4bKmHpEBJTX2dUuvUjSrr7vm2xvmuGYs3sPXdo+KGlxyV8KPNpXrrc21vtle2r+tBLUIyKKY5j98qbtFU1VOyDP0+RPK8MvERE9zY2pD/JGb8nx8nmo5A9bG2t/2Z6aP60E9YgIqIL15GS99NFsA9aV7XXA/X35ayWdIOl8qhuij5ehmvckrSqzXq7vO2aoDL9ERPQ09AIMSfcAV1CNve8H/hy4Fdgq6QbgNeA6ANt7JG0Fnqd6k9zNtnsvk7iJaibNScADJU0rQT0iomjqiVLbXx2y68oh5TcCGwfk7wIuOpa6E9QjIno68ERpgnpEBJQpjQnqEREd4VFugrZGgnpERE+GXyIiOiLDLxERXWJwhl8iIrojwy8RER2R4ZeIiI7J7JeIiK4YabGu1khQj4iAMvySnnpERHd0oKc+8tK7ks6V9EtJeyXtkfTNAWWukPSOpKdL+rNR642IaNzsrqc+Fk301I8C37b9lKRTgCcl7bD9/JRyv7b95Qbqi4iYBc7sF6jetQf03rv3nqS9VO/RmxrUIyLay+CJiZnLtVyjbz6SdB5wCfDYgN2fk/SMpAckXdhkvRERjcjwywckfRz4MfAt2+9O2f0U8CnbhyVdA/yU6pVNg86zHlhfvh5ZsGTfc0218SNaBLw5x20A/l0b2tGKNrww922AllyLFrQB2tGOT498ht7r7Oa5RoK6pIVUAf1Htn8ydX9/kLe9XdIdkhbZ/tAPgu1NwKZy3l0NvrH7I2lDG9rSjrShXe1oQxva0g5Juxo5Uct74XWMHNTLC1F/AOy1/b0hZc4B3rBtSSuphn3eGrXuiIgmOT11AC4HvgbslvR0yfsO8EkA23cBXwFuknQU+AOw1u7AP4kR0R02TCSoY/s3gGYocztw+0c4/aaP1KhmtaEN0I52pA0faEM72tAGaEc7mmlDB5beVTrMERFw6nFnetXxV9cqu+P9e56c6/sIw2SZgIgIKNMV539PvdF56qOQdIakHZJeKp+nDyn3qqTdZbmBZu54V+ddLelFSfskbRiwX5JuK/uflXRpU3UfQxtmfbkFSXdLOiRp4FTScVyHmu0Yx7WoswTGrF6PtizDIelESY+XZ032SPqLAWVm+1rUacNI18KTrpVazXYrEvCfgA1lewPwH4eUexVY1HDdC4DfAv8Y+BjwDHDBlDLXAA9Q3T9YBTw2B224AvjZLP89/FPgUuC5Iftn9TocQzvGcS2WAJeW7VOA/zsHPxd12jCOayHg42V7IdUDhqvGfC3qtOEjXwvgQWBXzfTgbF7vUVKbhl/WUP2FAGwGfgX86ZjqXgnss/0ygKR7S3v6lzpYA2xx9bf/qKTTJC1xtUzCuNow62w/rOrJ4GFm+zrUbcesc70lMGb1etRsw6wrf77D5evCkqZ2WWf7WtRpwyjnX93UueZSa4ZfgLN7f/nlc/GQcgZ+LulJVU+fNmEp8Hrf9/0l71jLzHYbYO6XW5jt63AsxnYtNHwJjLFdj2naAGO4FpIWqJq2fAjYYXvs16JGG2Duf0fm1Fh76pJ+AZwzYNd3j+E0l9s+IGkxsEPSC7YfHrVpA/Km9gDqlJntNtRebmEWzfZ1qGts10LTL4ExlusxQxvGci1sTwAXSzoNuE/SRbb773nM+rWo0YY2/I7MqbH21G1/wfZFA9L9wBuSlgCUz0NDznGgfB4C7qMathjVfuDcvu/LgAMfocystsH2u7YPl+3twEJJixpsQx2zfR1qGde10AxLYDCG6zFTG8b9c2H7b6mGR6cOV4ztZ2NYG1ryOzKn2jT8sg1YV7bXAfdPLSDpZFVrtiPpZOCLQBMLfj0BLJd0vqSPAWtLe6a27/pyh38V8E7D48gztkHSOZJUtudquYXZvg61jONalPNPuwQGs3w96rRhTNfirNI7RtJJwBeAF6YUm+1rMWMbWvI7MqfadKP0VmCrpBuA14DrACR9Avi+7WuAs6n+ywVV2//K9oOjVmz7qKRvAA9RzUK52/YeSTeW/XcB26nu7u8Dfg98fdR6P0IbZn25BUn3UN2wXiRpP/DnVDekxnIdjqEd41h6os4SGLN9PdqyDMcSYLOkBVSBcqvtn43zd6RmG/7BL0mSJ0ojIjqkTcMvERExogT1iIgOSVCPiOiQBPWIiA5JUI+I6JAE9YiIDklQj4jokAT1iIgO+X96eo9CQ/uHtgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(np.mean(test_mse_min, axis=2))\n",
    "print(np.mean(test_mse_ose, axis=2))\n",
    "print('-----------------------------------')\n",
    "print(np.mean(train_mse_min, axis=2))\n",
    "print(np.mean(train_mse_ose, axis=2))\n",
    "\n",
    "plt.imshow(np.mean(test_mse_min, axis=2))\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.mse_path_.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the feature selection capability\n",
    "\n",
    "Compute sensitivity and specificity using the 0-1 codings of the true and estimated coefficient vectors and plot them in a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Questions\n",
    "\n",
    " - What differences between sensitivity/specificity computed from the $λ_{min}$ and the $λ_{1se}$ models can you observe?\n",
    " - How do different choices for n and sparsity affect the relationship of sensitivity/specificity?"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2043299c89c8cd0b4d1a6f5cf4529bd58e6a4e0fe3181a25e0d328c821cdc5c5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
