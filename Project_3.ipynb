{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Prediction quality vs feature selection\n",
    "\n",
    "#### Tasks\n",
    "\n",
    "In this taks you are supposed to\n",
    "\n",
    "1. repeatedly simulate new datasets and for each dataset do Steps 2 and 3 \n",
    "2.  determine λ_{min} and λ_{1se} using cross-validation on the training data\n",
    "\n",
    "3.  compare the two resulting Lasso models with respect to\n",
    "    - Mean squared error on the training as well as test data\n",
    "    - Feature selection quality in comparison to the original simulated regression coefficients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look a bit closer at each task.\n",
    "\n",
    "1. Simulate data\n",
    "Simulating useful data for the Lasso isn't complicated but to make sure you can focus on the interesting tasks I included a Python and a R version of a data-generating function. You can either ignore them and follow the description here, use them or take them as inspiration to write your own. The ideas are\n",
    "\n",
    "    1. Generate n observations from an isometric normal distribution $N(0,Ip)$, i.e. theoretically uncorrelated features.\n",
    "    2. Generate regression coefficients beta as a vector that contains ceiling((1 - sparsity) p) non-zero elements [ceiling = closest integer less or equal to] that are normal distributed with a standard deviation of beta_scale. All other elements are zero.\n",
    "    3. At this point Xβ is the noise-less response and √∥Xβ∥22/(n−1) is its sample standard deviation. Signal-to-noise ratio is a measure for the proportion of the signal (noise-less response) standard deviation to the standard deviation of the noise, i.e. SNR = sd_signal / sd_noise. Specifying the SNR is a convenient way to determine what standard deviation for the noise is reasonable by choosing sd_noise = sd_signal / SNR. As an example, SNR = 2 means that the standard deviation of the noise-less is twice as large as the standard deviation of the noise and the noise-less response will be more \"pronounced\" the larger the SNR is. If 0 <= SNR < 1, then the noise is stronger than the signal which is hard to deal with for most methods.\n",
    "    4. Finally, the response is created in the form of a linear model y = X beta + sigma eps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_data(n, p, rng, *, sparsity=0.95, SNR=2.0, beta_scale=5.0):\n",
    "    \"\"\"Simulate data for Project 3, Part 1.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Number of samples\n",
    "    p : int\n",
    "        Number of features\n",
    "    rng : numpy.random.Generator\n",
    "        Random number generator (e.g. from `numpy.random.default_rng`)\n",
    "    sparsity : float in (0, 1)\n",
    "        Percentage of zero elements in simulated regression coefficients\n",
    "    SNR : positive float\n",
    "        Signal-to-noise ratio (see explanation above)\n",
    "    beta_scale : float\n",
    "        Scaling for the coefficient to make sure they are large\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : `n x p` numpy.array\n",
    "        Matrix of features\n",
    "    y : `n` numpy.array\n",
    "        Vector of responses\n",
    "    beta : `p` numpy.array\n",
    "        Vector of regression coefficients\n",
    "    \"\"\"\n",
    "    X = rng.standard_normal(size=(n, p))\n",
    "    \n",
    "    q = int(np.ceil((1.0 - sparsity) * p))\n",
    "    beta = np.zeros((p,), dtype=float)\n",
    "    beta[:q] = beta_scale * rng.standard_normal(size=(q,))\n",
    "    \n",
    "    sigma = np.sqrt(np.sum(np.square(X @ beta)) / (n - 1)) / SNR\n",
    "\n",
    "    y = X @ beta + sigma * rng.standard_normal(size=(n,))\n",
    "\n",
    "    # Shuffle columns so that non-zero features appear\n",
    "    # not simply in the first (1 - sparsity) * p columns\n",
    "    idx_col = rng.permutation(p)\n",
    "    \n",
    "    return X[:, idx_col], y, beta[idx_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 500     # Fix p at something large, e.g. 500 or 1000\n",
    "n_list = [125, 250, 375]    # Let n vary compared to p, e.g. iterate through [200, 500, 750] if you set p = 1000. What truly matters here is the ratio p / n, so if you choose p differently, adjust your choices for n\n",
    "sparsities = [0.75, 0.9, 0.95, 0.99]    # Let sparsity vary for a few choices, e.g. [0.75, 0.9, 0.95, 0.99]\n",
    "SNR = 2     # You can fix SNR at something reasonable like 2 or 5 throughout\n",
    "beta_scale = 5      # Same holds for beta_scale, maybe 5 or 10\n",
    "rng = np.random.default_rng(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, it will help you tremendously in the interpretation of the results if you **repeat the simulations a few times**, say 5 or 10 times, for each choice of n and sparsity. Here is why you should be careful with your choices: If you chose three values for n and four for sparsity, as well as 5 repeats, then you need to run your simulations for 60 datasets. A setup with 50 datasets took about 2 minutes on a 2017 MacBook, so if it takes hours, you did something wrong :-)\n",
    "\n",
    "It can be good to include intermediate print-outs/clock output throughout the code, e.g. for iteration numbers or cross-validation so you can detect if there is a time sink somewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, beta = simulate_data(n_list[0], p, rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit Lasso model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.828751249920303"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = LassoCV(cv=5, random_state=0).fit(x, y)\n",
    "alpha = reg.alpha_\n",
    "reg.score(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.828751249920303"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_2 = Lasso(alpha=alpha).fit(x,y)\n",
    "reg_2.score(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.mse_path_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Determine hyperparameters\n",
    "\n",
    "This works as described above the \"Tasks\" section. Depending on the the package you are using you will have to perform some different steps to get the coefficients and the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.752793759040847\n",
      "18.41228025572191\n",
      "19.87550066554975\n",
      "17.84007855660928\n",
      "8.451319633325255\n",
      "2.309117994413889\n",
      "4.168316154069316\n",
      "5.7573150863919\n",
      "1.9674058324493704\n",
      "4.686999341335015\n",
      "3.050204070587627\n",
      "3.1091374245480536\n",
      "3.2033887988794465\n",
      "3.1246407717523303\n",
      "1.4556675590140313\n",
      "1.3383695086334695\n",
      "1.2779497688572923\n",
      "0.9634235300674979\n",
      "1.6688346127258922\n",
      "0.9667837279390837\n",
      "2.6504013999173033\n",
      "1.9798582660277964\n",
      "2.812765470480938\n",
      "2.5879015713764555\n",
      "4.239692423135225\n",
      "2.3214343541863203\n",
      "1.9621086857561538\n",
      "2.0447277846167418\n",
      "1.413529001738004\n",
      "1.908024730129443\n",
      "0.9216855160251751\n",
      "1.6288585461686438\n",
      "1.2640874836169973\n",
      "1.5483647807962941\n",
      "1.0351721931081137\n",
      "0.851909638110247\n",
      "0.6848998542167665\n",
      "0.5241395457717647\n",
      "0.8683217402408382\n",
      "0.9408574588691055\n",
      "1.394420401052372\n",
      "1.6425937379641709\n",
      "1.6967259321789236\n",
      "1.2134049764357242\n",
      "1.966211360111008\n",
      "1.2840264382529107\n",
      "1.985171685158749\n",
      "1.8387763880079742\n",
      "1.2877046270520742\n",
      "0.931159324422454\n",
      "0.9015365884566127\n",
      "1.2101449474325652\n",
      "1.59729162187304\n",
      "1.0547407587872262\n",
      "0.9185988937552826\n",
      "0.37396438307733043\n",
      "0.845374560531673\n",
      "0.6808600113209367\n",
      "0.8554716331938783\n",
      "1.5994640439939352\n"
     ]
    }
   ],
   "source": [
    "for n in n_list:\n",
    "    for sparsity in sparsities:\n",
    "        for i in range(5):\n",
    "            x, y, beta = simulate_data(n, p, rng, sparsity=sparsity)\n",
    "            lasso = LassoCV(cv=5).fit(x, y)\n",
    "            mse_mat = lasso.mse_path_\n",
    "            n_folds = mse_mat.shape[0]\n",
    "            cv_mean = np.mean(mse_mat, axis=1)\n",
    "            cv_std = np.std(mse_mat, axis=1)\n",
    "            idx_min_mean = np.argmin(cv_mean)\n",
    "            idx_alpha = np.where(\n",
    "                (cv_mean <= cv_mean[idx_min_mean] + cv_std[idx_min_mean] / np.sqrt(n_folds)) &\n",
    "                (cv_mean >= cv_mean[idx_min_mean])\n",
    "            )[0][0]\n",
    "            alpha_1se = lasso.alphas_[idx_alpha]\n",
    "            print(alpha_1se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.mse_path_.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Comparing the two Lasso models\n",
    "\n",
    "#### Computation of train/test MSE\n",
    "\n",
    "Whenever you use LassoCV or cv.glmnet you can, at the best, access the test error for each fold. To compute both the training and test MSE of the model, you have two options:\n",
    "\n",
    "1. Write your own cross validation loop. Then you can evaluate the training and test error and safe both.\n",
    "2. Instead of only creating a training dataset with the options given in Part 1, Task 1 you can do the following:\n",
    "    - Fix n_test at some larger value, say, n_test = 500 or n_test = 1000\n",
    "    - Instead of simulating n samples you generate now n + n_test and split the dataset. Train on the n samples with cross validation and use the remaining n_test samples for testing. You do not need to adapt the size of the test set to the size of the training dataset as long as you choose it large enough.\n",
    "Option 2 is probably less tedious to implement, but the choice is up to you.\n",
    "\n",
    "When reporting the results for the train/test MSE, **please do not simply report a table of numbers. Find a nice way to visualise the results.**\n",
    "\n",
    "#### Question\n",
    " - How does the MSE of the $λ_{min}$ and $λ_{1se}$ models behave for different n and sparsity levels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the feature selection capability\n",
    "\n",
    "Compute sensitivity and specificity using the 0-1 codings of the true and estimated coefficient vectors and plot them in a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Questions\n",
    "\n",
    " - What differences between sensitivity/specificity computed from the $λ_{min}$ and the $λ_{1se}$ models can you observe?\n",
    " - How do different choices for n and sparsity affect the relationship of sensitivity/specificity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Selecting features with confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn')\n",
    "import sys\n",
    "\n",
    "if sys.platform == 'darwin':\n",
    "    data = pd.read_csv('TCGA-PANCAN-HiSeq-801x20531/data.csv', index_col=0)\n",
    "    labels = pd.read_csv('TCGA-PANCAN-HiSeq-801x20531/labels.csv', index_col=0)\n",
    "else:\n",
    "    data = pd.read_csv('TCGA-PANCAN-HiSeq-801x20531\\data.csv', index_col=0)\n",
    "    labels = pd.read_csv('TCGA-PANCAN-HiSeq-801x20531\\labels.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Nisse/Library/Python/3.8/lib/python/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [    5    23  4370  4808  4809  4814  4816  4817  4831  5288  7661  7662\n",
      "  7663  7664  7665  8121  9304  9306  9314  9316  9320  9452 10121 11958\n",
      " 13991 14158 14159 14161 15138 15140 15141 15446 16566 16568 16569 16571\n",
      " 16575 16578 16579 16604 16634 16637 16677 16697 16698 16699 16700 16701\n",
      " 16702 16704 16705 16706 16707 16708 16709 16710 16711 16712 16713 16714\n",
      " 16715 16716 16717 16718 16719 16720 16721 16722 16723 16724 16725 16726\n",
      " 16727 16728 16729 16730 16731 16732 16733 16734 16735 16736 16737 16738\n",
      " 16739 16740 16741 16742 16743 16744 16745 16746 16748 16749 16750 16751\n",
      " 16752 16753 16754 16756 16757 16758 16759 16760 16761 16762 16763 16764\n",
      " 16765 16766 16767 16768 16769 16770 16771 16772 16774 16775 16776 16777\n",
      " 16778 16779 16780 16781 16782 16783 16785 16787 16788 16789 16790 16791\n",
      " 16792 16794 16795 16796 16798 16799 16800 16801 16802 16803 16804 16805\n",
      " 16806 16807 16808 16809 16810 16811 16812 16813 16816 16818 16819 16820\n",
      " 16821 16822 16823 16824 16826 16827 16830 16831 16832 16833 16834 16835\n",
      " 16836 16837 16838 16839 16840 16841 16842 16843 16844 16845 16846 16847\n",
      " 16848 16849 16850 16851 16852 16853 16854 16855 16856 16857 16858 16859\n",
      " 16860 16861 16862 16863 16864 16865 16866 16867 16868 16869 16870 16871\n",
      " 16872 16873 16874 16875 16876 16877 16878 16879 16880 16881 16882 16883\n",
      " 16884 16885 16886 16888 16889 16890 16891 16892 16893 16894 16895 16896\n",
      " 16897 16898 16899 16900 16901 16902 16903 16904 16905 16906 16907 16908\n",
      " 16909 16910 16911 16914 16915 16916 16917 16918 16920 16921 16922 16924\n",
      " 16925 16926 18829 18902 18903 18908 18909 18910 18911 18914 18915 19450\n",
      " 19451 19452 19671] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/Users/Nisse/Library/Python/3.8/lib/python/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "labels = (np.asarray(labels)).reshape(len(labels,))\n",
    "k_features = 200\n",
    "data_filtered = SelectKBest(k=k_features).fit_transform(data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performe feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "hp = np.logspace(-4, 4, 30)\n",
    "logresCV = LogisticRegressionCV(Cs=hp, cv=5, multi_class='ovr', solver='liblinear', intercept_scaling=1000, scoring='f1').fit(data_filtered, labels)\n",
    "# logresCV.predict(data_filtered)\n",
    "scoresCV = np.argmax(logresCV.scores_)\n",
    "C_max = np.max(logresCV.C_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STD for each class: [[0.00404874 0.00404874 0.00404874 0.00404874 0.00404874 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.01618685 0.01580316 0.01580316 0.01290323 0.01290323 0.01290323\n",
      "  0.01290323 0.01290323 0.01290323 0.01290323 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.01419927 0.01428571 0.01428571 0.00701754 0.00701754 0.00701754\n",
      "  0.00701754 0.00701754 0.00701754 0.00701754 0.00701754 0.00701754\n",
      "  0.00701754 0.00701754 0.00701754 0.00701754 0.00701754 0.00701754\n",
      "  0.00701754 0.00701754 0.00701754 0.00701754 0.00701754 0.00701754\n",
      "  0.00701754 0.00701754 0.00701754 0.00701754 0.00701754 0.00701754]\n",
      " [0.02994365 0.01921567 0.02250392 0.02250392 0.02250392 0.02250392\n",
      "  0.00890724 0.00890724 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]]\n",
      "Max score over classes: [ 5. 10.  3.  8.  0.]\n",
      "Lower bound for one standard deviation from maximum score: [1.         1.         0.98947368 1.         1.        ]\n",
      "Minimum mean score over classes: [1.         1.         0.99285714 1.         1.        ]\n",
      "Minimum mean position over classes: [ 5. 10.  1.  8.  0.]\n",
      "C_1se: [0.00239503 0.05736153 0.00018874 0.01610262 0.0001    ]\n",
      "C_max: [0.00239503 0.05736153 0.00067234 0.01610262 0.0001    ]\n"
     ]
    }
   ],
   "source": [
    "# Cmax\n",
    "C_max = logresCV.C_\n",
    "# C_1se\n",
    "argmax_over_classes = np.zeros(5)\n",
    "sub_std = np.zeros(5)\n",
    "mean_over_folds = np.zeros([5, 30])\n",
    "std_over_classes = np.zeros([5, 30])\n",
    "min_mean_over_classes = np.zeros(5)\n",
    "where_min_mean = np.zeros(5)\n",
    "for idx, key in enumerate(logresCV.scores_.keys()):\n",
    "    current_class_score = logresCV.scores_[key]\n",
    "    mean_over_folds[idx,:] = np.mean(current_class_score, axis=0)\n",
    "    argmax_over_classes[idx] = np.argmax(mean_over_folds[idx,:])\n",
    "    std_over_classes[idx,:] = np.std(current_class_score, axis=0)\n",
    "    sub_std[idx] = mean_over_folds[idx, int(argmax_over_classes[idx])] - std_over_classes[idx,int(argmax_over_classes[idx])]\n",
    "    min_mean_over_classes[idx] = np.min(mean_over_folds[idx,:][mean_over_folds[idx,:] >= sub_std[idx]])\n",
    "    where_mean = np.where(mean_over_folds[idx,:] >= sub_std[idx])\n",
    "#     and (mean_over_folds[cl,:] <= np.ones(30)*max_over_classes[cl]\n",
    "#                idx_alpha = np.where(\n",
    "#                (cv_mean <= cv_mean[idx_min_mean] + cv_std[idx_min_mean] / np.sqrt(n_folds)) &\n",
    "#                (cv_mean >= cv_mean[idx_min_mean])\n",
    "#            )[0][0]\n",
    "    where_min_mean[idx] = where_mean[0][int(np.argmin(mean_over_folds[idx, where_mean]))]\n",
    "print('STD for each class:', std_over_classes)\n",
    "print('Max score over classes:', argmax_over_classes)\n",
    "print('Lower bound for one standard deviation from maximum score:', sub_std)\n",
    "print('Minimum mean score over classes:', min_mean_over_classes)\n",
    "print('Minimum mean position over classes:', where_min_mean)\n",
    "C_1se = hp[where_min_mean.astype(int)]\n",
    "print('C_1se:', C_1se)\n",
    "print('C_max:', C_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_of_counts = np.zeros([5, k_features])\n",
    "M = 100\n",
    "for m in range(M):\n",
    "    indicies = np.random.choice(len(labels), len(labels), replace=True)\n",
    "    bootstrap_data = data_filtered[indicies, :]\n",
    "    bootstrap_labels = labels[indicies]\n",
    "    logres = LogisticRegression(C=np.max(C_max), penalty='l1', multi_class='ovr', solver='liblinear', random_state=0, intercept_scaling=1000).fit(bootstrap_data, bootstrap_labels)\n",
    "    logres.predict(bootstrap_data)\n",
    "    scores = logres.score(bootstrap_data, bootstrap_labels)\n",
    "    coefficients = logres.coef_\n",
    "    for genetype in range(5):\n",
    "        features = np.where(coefficients[genetype, :] != 0)\n",
    "        table_of_counts[genetype, features[0]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,   0.,   0.,   0.,  20.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,  65.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,  76.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,  31.,   0.,   0.,   0.,   0., 100.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 100.,  12.,\n",
       "         0.,   0.,   0.,   0.,  32.,   0.,   0.,   0.,   0.,   0.,  99.,\n",
       "         0.,   0.,   0.,  97.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,  64.,   0.,\n",
       "         0.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,  20.,   0.,  93.,  14.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0., 100.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,  99.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,  67.,   6.,   0.,   0.,\n",
       "         0.,   0.])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for m in range(M):\n",
    "    for gene in range(5):\n",
    "        np.where(table_of_counts[gene] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.shape(data_filtered)\n",
    "indicies = np.random.choice(len(labels), len(labels), replace=True)\n",
    "bootstrap_data = data_filtered[indicies, :]\n",
    "bootstrap_labels = labels[indicies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  9.70084889, ...,  3.57740466,\n",
       "         2.69347581,  4.7897136 ],\n",
       "       [ 0.        ,  0.        ,  9.50103701, ...,  0.53923327,\n",
       "         0.53923327,  0.53923327],\n",
       "       [10.15113073,  8.16893782, 12.45731162, ..., 11.19681725,\n",
       "         9.88208512, 11.68032875],\n",
       "       ...,\n",
       "       [ 0.44911232,  0.        ,  8.52509792, ...,  1.29918603,\n",
       "         1.06743221,  2.21794413],\n",
       "       [ 1.11196609,  0.        ,  7.76687339, ...,  2.28436607,\n",
       "         2.03463832,  2.03463832],\n",
       "       [ 0.        ,  0.        ,  8.60620161, ...,  0.        ,\n",
       "         2.83669169,  0.49528583]])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bootstrap_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390 5\n"
     ]
    }
   ],
   "source": [
    "# counts = np.bincount(k)\n",
    "values, count = np.unique(k, return_counts=True)\n",
    "print(values[np.argmax(count)], np.max(count))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a20783d42108fef31dde0a57614c05dbf2c147ed65f20fcc3e2fe997a3c36b6e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
